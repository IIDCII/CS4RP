{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Test \n",
    "Making a test for fine tuning for planning projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from huggingface_hub import login, snapshot_download, hf_hub_download\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check VRAM\n",
    "# if not using Cheery, you will need to change this\n",
    "def check_vram():\n",
    "    # Initialize NVIDIA management library\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "    # Get a handle for each GPU device\n",
    "    handle_list = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(pynvml.nvmlDeviceGetCount())]\n",
    "\n",
    "    info_used = []\n",
    "    points = [0,1,2,3,4,5,6,7]\n",
    "    max_vram = 24\n",
    "\n",
    "    # Iterate over all GPU devices and print VRAM usage\n",
    "    for handle in handle_list:\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        info_used.append( (info.used//1024**2)/1000)\n",
    "\n",
    "    print (info_used)\n",
    "\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.ylim(0,max_vram)\n",
    "    plt.bar(points, info_used)\n",
    "    plt.plot()\n",
    "    pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT RUN THIS CODE BLOCK IF 'llama-2-7b' IS IN YOUR DIRECTORY\n",
    "\"\"\"\n",
    "\n",
    "# getting the authorisation from huggingface to use the model (in this case llama 2 7b )\n",
    "access_key = open('hf_ak.txt','r').read()\n",
    "login(token = access_key)\n",
    "\n",
    "# Download model files. If it's in the project directory, there's no need to run this again\n",
    "# use df -H in the terminal to check and see if there's enough space to download the model\n",
    "# this process will take a long time\n",
    "# will save the model in the directory specified\n",
    "model_path = snapshot_download(\"meta-llama/Llama-2-7b\", local_dir=\"./llama-2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do not run any of this is if llama-2-7b-hf is filled\n",
    "- You'll need to convert them to the huggingface Transformers format using the conversion script `convert_llama_weights_to_hf.py`. \n",
    "- Obviously, hf stands for huggingface. Maybe with the hf version, the conversion wouldn't be needed\n",
    "- to run this, run the following\n",
    "\n",
    "```\n",
    "python convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /llama-2-7b --model_size 7B --output_dir /llama-2-7b-hf\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.079, 5.791, 0.418, 4.677, 0.418, 0.418, 0.418, 0.418]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utilization.gpu [%]\n",
      "0 %\n",
      "0 %\n",
      "0 %\n",
      "0 %\n",
      "0 %\n",
      "0 %\n",
      "0 %\n",
      "0 %\n"
     ]
    }
   ],
   "source": [
    "# each GPU util\n",
    "!nvidia-smi --query-gpu=utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.88s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "# pointing to the hf version of the model\n",
    "model_path = \"llama-2-7b-hf\"\n",
    "\n",
    "# device_map auto -> distribute data and compute across the GPU's automatically\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map = \"sequential\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "# using the tokenizer that this model has\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.404, 9.588, 0.625, 4.884, 0.625, 0.625, 0.625, 0.625]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# making sure model gets loaded correctly\n",
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to clear VRAM\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to clear VRAM\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing the og weights\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init lora adapters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8388608 || all params: 6746804224 || trainable%: 0.12433454005023165\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 824/824 [00:01<00:00, 452.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# getting the data\n",
    "# getting the dataset from hugging face library\n",
    "# using this so we don't have to reformat which we'll get into later\n",
    "ds = load_dataset(\"google/frames-benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds[\"test\"]\n",
    "\n",
    "# prompts\n",
    "data_x = data\n",
    "\n",
    "# answers to the prompts\n",
    "data_y = data\n",
    "\n",
    "train_data, temp_data, train_y, temp_y = train_test_split(data_x, data_y, test_size=0.3, random_state=30)\n",
    "val_data, test_data, val_y, test_y = train_test_split(temp_data, temp_y, test_size=0.5, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m----> 3\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[1;32m      5\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[1;32m      6\u001b[0m         gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      7\u001b[0m         warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      9\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m,\n\u001b[1;32m     10\u001b[0m         fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     12\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m     ),\n\u001b[1;32m     14\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[1;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/mnt/faster2/dc903/CS4RP/fttenv/lib/python3.12/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=ds['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
