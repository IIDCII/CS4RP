{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Test \n",
    "Test for fine tuning language models using LoRa on Bath's Hex Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up on Hex Cloud\n",
    "## Getting Access\n",
    "- if you don't have permission to a cluster/machine, email Tom Haines\n",
    "- open a terminal and sign in via ssh and input your password\n",
    "    ```\n",
    "    ssh uniusername@clustername.cs.bath.ac.uk\n",
    "    ```\n",
    "    If the following doesn't work, make sure that you're in Bath and if not, connect to the university's VPN or any other VPN\n",
    "\n",
    "## Access project folders\n",
    "- once you're in you want to navigate to the the fast/er folders. You want to do all of your projects there because they're the least laggy and you have up to 3.8T of storage for your project (varies)\n",
    "\n",
    "- <img src=\"assets/s1.png\" width = \"300\">\n",
    "\n",
    "- the following is an example of how you would access the fast/faster project files\n",
    "- check which of the files has the most space and create a folder with your username\n",
    "- do not put any confidential information or API keys within this folder. If you want to use that, put that in your home folder and call them from your project folder\n",
    "\n",
    "## Getting Access via VsCode\n",
    "- open VsCode and in the bottom left corner, you'll get the option to open a remote connection\n",
    "- then select `Connect Current Window to Host...`\n",
    "- if the configured ssh you want to use is not there, then select `Add New SSH Host`\n",
    "    - if you've selected add new host, input `uniusername@clustername.cs.bath.ac.uk`\n",
    "    - else, select the ssh connection you want\n",
    "- if you're experiencing issues make sure that you're either in uni or you have a VPN turned on\n",
    "\n",
    "## Connecting to GitHub\n",
    "- to clone a repo on a server (using ssh) using GitHub follow the tutorial [here](https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/github-clone-with-ssh-keys)\n",
    "- remember to save and store all public or private keys within your home directory\n",
    "\n",
    "## Transferring files\n",
    "### Using VsCode\n",
    "- drag files into the working tab directory in VsCode\n",
    "- probably the best way to quickly transfer files to the Cloud\n",
    "### Using Github\n",
    "- this should be straight forward as you can just clone your project in your local directory\n",
    "### Using CyberDuck (Mac)\n",
    "- download on the internet, have to pay via app store\n",
    "- connect via FTAP and there's no need to change the port number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up ipynb file for Tuning\n",
    "\n",
    "Once everything is set up on the server or on your own machine, you want to firstly set/assign what GPUs we're going to be using in the cluster. We need to do this first before setting up everything else so it restricts anything from using other GPU's. Check the [usage](https://hex.cs.bath.ac.uk/usage) of the current cluster you're working on and change the GPUs you're using based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important to run this first or else GPU allocation will not work\n",
    "import os\n",
    "# set the GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "# setting for vllm inference so that it can run in parallel\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding all of the neccessary imports\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, TrainingArguments, pipeline, BitsAndBytesConfig\n",
    "from huggingface_hub import login, snapshot_download, hf_hub_download\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "import torch.nn as nn\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "# checking if GPU allocation is successful\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.518, 5.791, 13.515, 17.308, 0.415, 0.415, 0.418, 0.418]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check VRAM\n",
    "# you will need to alter this code if you're not using the Cheery cluster on Hex\n",
    "def check_vram():\n",
    "    # Initialize NVIDIA management library\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "    # Get a handle for each GPU device\n",
    "    handle_list = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(pynvml.nvmlDeviceGetCount())]\n",
    "\n",
    "    info_used = []\n",
    "    points = [0,1,2,3,4,5,6,7]\n",
    "    max_vram = 24\n",
    "\n",
    "    # Iterate over all GPU devices and print VRAM usage\n",
    "    for handle in handle_list:\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        info_used.append( (info.used//1024**2)/1000)\n",
    "\n",
    "    print (info_used)\n",
    "\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.ylim(0,max_vram)\n",
    "    plt.bar(points, info_used)\n",
    "    plt.plot()\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the model\n",
    "For this example we're going to be fine tuning the meta llama-2-7b model that's being downloaded from huggingface. In order to do this though you'll need to go through some preliminary steps\n",
    "\n",
    "- request access to the llama-2-7b (recommended to get llama-2-7b-hf so you don't have to convert it later) model. This should take about an hour or so depending on when you've sent the request\n",
    "- create an access key for your huggingface account. Follow this tutorial [here](https://huggingface.co/docs/hub/en/security-tokens) for more info. Make sure that you give the key read and write permissions\n",
    "- upload your key into a txt file and store it onto the server. As shown in this test it's in the project directory which is not good practice. Store it in your home directory and replace the path\n",
    "- if you need to convert your model to the huggingface (hf) version, run the `convert_llama_weights_to_hf.py`. This will only work for llama models\n",
    "\n",
    "If all steps are done then the next code block should work smoothly but will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT RUN THIS CODE BLOCK IF 'llama-2-7b-hf' IS IN YOUR DIRECTORY\n",
    "\"\"\"\n",
    "\n",
    "# getting the authorisation from huggingface\n",
    "access_key = open('hf_ak.txt','r').read()\n",
    "login(token = access_key)\n",
    "\n",
    "# use df -H in the terminal to check and see if there's enough space to download the model\n",
    "# will save the model in the directory specified\n",
    "# for future cases it's recommended to download \"meta-llama/Llama-2-7b-hf\" instead of \"meta-llama/Llama-2-7b\" as you will not need to convert it later\n",
    "model_path = snapshot_download(\"meta-llama/Llama-2-7b-hf\", local_dir=\"./llama-2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do not run any of this is if llama-2-7b-hf is filled\n",
    "- You'll need to convert them to the huggingface Transformers format using the conversion script `convert_llama_weights_to_hf.py`. \n",
    "- Obviously, hf stands for huggingface. Maybe with the hf version, the conversion wouldn't be needed\n",
    "- to run this, run the following\n",
    "\n",
    "```\n",
    "python convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /llama-2-7b --model_size 7B --output_dir /llama-2-7b-hf\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each GPU util\n",
    "!nvidia-smi --query-gpu=utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"llama-2-7b-hf\"\n",
    "new_model_name = \"llama-2-7b-enhanced\" #You can give your own name for fine tuned model\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use the following code if you want to unload the VRAM. You may have to run this more than once to unload\n",
    "Remove model from GPU. Add any more variables that can get loaded on\n",
    "\"\"\"\n",
    "\n",
    "# uncomment this if not deleted\n",
    "# del base_model\n",
    "# del llama_tokenizer\n",
    "\n",
    "# Clear any remaining CUDA memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if your project is still loaded onto the VRAM\n",
    "print(torch.cuda.memory_allocated())\n",
    "# this should return 0 if everything is unloaded\n",
    "print(torch.cuda.memory_reserved())\n",
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "As we're already connected to our huggingface account, you can also download datasets from there too which is what we're doing. This dataset specifically is made for the llama 2 models. \n",
    "\n",
    "You need to make sure the data is configured to the data you want. If you want to use data that has no configuration for your chosen model, then reconfigure the data. I will not be doing that in this file but I'll leave a link to example code if I get round to it\n",
    "\n",
    "For the llama 2 data formatting, the data is classed under `text` and starts on the `<s>` token and ends on the `</s>` token. `[INST]` implies the query and `[/INST]` implies the response. As long as you have data following this format, fine tuning should be able to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set\n",
    "# only 1K datapoints but each points has a lot of data\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# note this dataset only has a test split and no test one\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "# check the data\n",
    "print(training_data.shape)\n",
    "# #11 is a QA sample in English\n",
    "print(training_data[11])\n",
    "print(training_data[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Once you have your model and the data you want to fine tune the model with, we'll start training. In this test, we are going to be using LoRa to make training a language model possible in the first place.\n",
    "\n",
    "## How LoRa Works:\n",
    "Full paper [here](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "LoRa works by reducing the amount of weight we're fine tuning so that large models can be able to fit onto smaller machines. It does this by using 2 matricies of weights with varying size (rank or r) to represent a hidden layer\n",
    "\n",
    "For example, if I want to train a single hidden layer with input size 400 and output size 600, then I can reduce that using 2 matrices: a with size `[input, rank]` and b with size `[rank, output]`. Therefore, if I get the dot product of ab, then the size would be `[input, ouput]`. Depending on what you set the rank to be (including other variables). This can significantly reduce the amount of weights that you're fine tuning. Obviously the less weights you're fine tuning, the less effect fine tuning has on the model. This can be good so you don't dramatically alter the pretraining but bad if it's not altering enough.\n",
    "\n",
    "Once the fine tuning is complete, then you add the fine tuned weights to the weights in the model which will give you your fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "# LoRA Config\n",
    "# reduce rank r if you're running out of vram\n",
    "peft_parameters = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with LoRA configuration\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the model\n",
    "\n",
    "Since the model that we saved is only the fine tuned weights without the pretrained ones, we need to merge them both together and save it. If llama-2-7b-merged is already in your directory, you will not need to do this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "# make sure that both models are available before running or else inference will not work\n",
    "base_model_name = \"llama-2-7b-hf\"\n",
    "new_model_name = \"llama-2-7b-enhanced\"\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"llama-2-7b-merged\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference / Testing \n",
    "\n",
    "if the models that you want to test are already saved, then just run the os environment setup,import and nvm check blocks and you can head straight down here\n",
    "\n",
    "For the test we're going to be gathering the perplexity score of the model based on the test data and the responses it's made to the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(\n",
    "    model_path: str,\n",
    "    tokenizer_path: str,\n",
    "    texts: List[str],\n",
    "    max_tokens: Optional[int] = None,\n",
    "    sliding_window: Optional[int] = None,\n",
    "    max_new_tokens: int = 30,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity using VLLM for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the merged model\n",
    "        texts: List of texts to evaluate\n",
    "        max_tokens: Maximum sequence length (optional)\n",
    "        sliding_window: Size of sliding window for attention (optional)\n",
    "    \n",
    "    Returns:\n",
    "        float: Average perplexity across all texts\n",
    "    \"\"\"\n",
    "    # Initialize VLLM with your model\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=2,  # Adjust based on your GPU setup\n",
    "        max_num_seqs=1,\n",
    "        tokenizer = tokenizer_path,\n",
    "        trust_remote_code= True,\n",
    "    )\n",
    "\n",
    "    # get sampling params for pp\n",
    "    pp_sampling_params = SamplingParams(\n",
    "        temperature=1.0\n",
    "        top_p=1.0,\n",
    "        max_tokens=max_new_tokens,\n",
    "        logprobs = True,\n",
    "    )\n",
    "    \n",
    "    # Set sampling parameters for generating responses\n",
    "    gen_sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_tokens=max_new_tokens,\n",
    "        # change this for the llama tokens\n",
    "        stop=[\"</s>\"],\n",
    "        logprobs=True,\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    pp_sum = 0\n",
    "    pp_total = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        # Generate response\n",
    "        gen_outputs = llm.generate([text], gen_sampling_params)\n",
    "        response = gen_outputs[0].outputs[0].text\n",
    "\n",
    "        # get pp\n",
    "\n",
    "        cum_logprob = gen_outputs[0].outputs[0].cumulative_logprob\n",
    "        tokens_length = len(gen_outputs[0].outputs[0].token_ids)\n",
    "        pp = np.exp(- cum_logprob/tokens_length)\n",
    "        \n",
    "        # Store results for this text\n",
    "        result = {\n",
    "            \"input\": text,\n",
    "            \"response\": response,\n",
    "            \"perplexity\": pp,\n",
    "            \"output\": gen_outputs,\n",
    "        }\n",
    "        results.append(result)\n",
    "        pp_sum += pp \n",
    "    \n",
    "    pp_total = pp_sum / len(texts)\n",
    "    \n",
    "    # Calculate average perplexity across all texts\n",
    "    \n",
    "    return pp_total , results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Test Data\n",
    "\n",
    "This will work pretty much the same to getting the training data where we will get the test split instead. Make sure to run the loading data block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this will not work for the following data that we're using for this project since\n",
    "it has no test split. Just use this as an example of what to do with a test split\n",
    "\"\"\"\n",
    "\n",
    "test_data = load_dataset(data_name, split=\"test\")\n",
    "\n",
    "small_test_set = test_data[1:2]\n",
    "\n",
    "print (small_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-23 16:38:35 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 10-23 16:38:35 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='llama-2-7b-merged', speculative_config=None, tokenizer='llama-2-7b-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=llama-2-7b-merged, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-23 16:38:35 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-23 16:38:35 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:38 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-23 16:38:39 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:39 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 10-23 16:38:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:39 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-23 16:38:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /homes/dc903/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /homes/dc903/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "INFO 10-23 16:38:39 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x738ec1bf5c10>, local_subscribe_port=53031, remote_subscribe_port=None)\n",
      "INFO 10-23 16:38:39 model_runner.py:1056] Starting to load model llama-2-7b-merged...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:39 model_runner.py:1056] Starting to load model llama-2-7b-merged...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55d715400f04c0a83d4b669e6c35f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:41 model_runner.py:1067] Loading model weights took 6.3096 GB\n",
      "INFO 10-23 16:38:41 model_runner.py:1067] Loading model weights took 6.3096 GB\n",
      "INFO 10-23 16:38:42 distributed_gpu_executor.py:57] # GPU blocks: 3698, # CPU blocks: 1024\n",
      "INFO 10-23 16:38:42 distributed_gpu_executor.py:61] Maximum concurrency for 2048 tokens per request: 28.89x\n",
      "INFO 10-23 16:38:45 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:45 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:45 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 16:38:45 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 16:38:45 custom_all_reduce.py:233] Registering 65 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:45 custom_all_reduce.py:233] Registering 65 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:45 model_runner.py:1523] Graph capturing finished in 1 secs.\n",
      "INFO 10-23 16:38:45 model_runner.py:1523] Graph capturing finished in 1 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s, est. speed input: 44.22 toks/s, output: 73.69 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 93.54 toks/s, output: 49.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s, est. speed input: 34.40 toks/s, output: 73.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-23 16:38:47 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=408220)\u001b[0;0m INFO 10-23 16:38:47 multiproc_worker_utils.py:240] Worker exiting\n",
      "------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average perplexity:  2.008890993591092\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run the perplexity measure\n",
    "model_path = \"llama-2-7b-merged\"\n",
    "tokenizer_path = \"llama-2-7b-merged\"\n",
    "\n",
    "# can replace this with test data\n",
    "# in the training data but just using this example for testing purposes\n",
    "test_texts = [\n",
    "        \"write me a 1000 words essay about deez nuts.\",\n",
    "        \"Escriba un programa en python que muestre, en orden inverso, las cifras de un numero entero positivo. Por ejemplo, si el numero es el 324, deberia mostrar por pantalla el 423.\",\n",
    "        \"Can you tell me how Claude manages to get such great results\",\n",
    "    ]\n",
    "\n",
    "ppl = calculate_perplexity(\n",
    "        model_path=model_path,\n",
    "        tokenizer_path = tokenizer_path,\n",
    "        texts=test_texts,\n",
    "        max_tokens=300,  # Adjust based on your model's context window\n",
    "    )\n",
    "\n",
    "pp_total = ppl[0]\n",
    "responses = ppl[1]\n",
    "\n",
    "print(\"---\"*10)\n",
    "print(\"\\n\"*5)    \n",
    "print(\"Average perplexity: \", ppl[0])\n",
    "print(\"\\n\"*5)    \n",
    "print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "\n",
      "2000 word essay xenophobia what is xenophobia.\n",
      "2000 word essay on\n",
      "------\n",
      "2.2574527658391386\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "## Código\n",
      "\n",
      "\n",
      "```python\n",
      "def inversa(numero):\n",
      "    return int(str(numero)[::\n",
      "------\n",
      "1.7799369586082143\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      " from his photos? I’ve seen some of his work and I’ve been really impressed.\n",
      "Claude is a very talented photograph\n",
      "------\n",
      "1.9892832563259237\n",
      "------------------------------------------------------------\n",
      "\n",
      " total average perplexity:  2.008890993591092\n"
     ]
    }
   ],
   "source": [
    "for i in responses:\n",
    "    print(\"---\"*20)\n",
    "    print(i[\"response\"])\n",
    "    print(\"---\"*2)\n",
    "    print(i[\"perplexity\"])\n",
    "    print(\"---\"*20)\n",
    "\n",
    "\n",
    "print (\"\\n total average perplexity: \", pp_total)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
