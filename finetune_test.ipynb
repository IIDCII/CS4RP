{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Test \n",
    "Test for fine tuning language models using LoRa on Bath's Hex Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up on Hex Cloud\n",
    "## Getting Access\n",
    "- if you don't have permission to a cluster/machine, email Tom Haines\n",
    "- open a terminal and sign in via ssh and input your password\n",
    "    ```\n",
    "    ssh uniusername@clustername.cs.bath.ac.uk\n",
    "    ```\n",
    "    If the following doesn't work, make sure that you're in Bath and if not, connect to the university's VPN or any other VPN\n",
    "\n",
    "## Access project folders\n",
    "- once you're in you want to navigate to the the fast/er folders. You want to do all of your projects there because they're the least laggy and you have up to 3.8T of storage for your project (varies)\n",
    "\n",
    "- <img src=\"assets/s1.png\" width = \"300\">\n",
    "\n",
    "- the following is an example of how you would access the fast/faster project files\n",
    "- check which of the files has the most space and create a folder with your username\n",
    "- do not put any confidential information or API keys within this folder. If you want to use that, put that in your home folder and call them from your project folder\n",
    "\n",
    "## Getting Access via VsCode\n",
    "- open VsCode and in the bottom left corner, you'll get the option to open a remote connection\n",
    "- then select `Connect Current Window to Host...`\n",
    "- if the configured ssh you want to use is not there, then select `Add New SSH Host`\n",
    "    - if you've selected add new host, input `uniusername@clustername.cs.bath.ac.uk`\n",
    "    - else, select the ssh connection you want\n",
    "- if you're experiencing issues make sure that you're either in uni or you have a VPN turned on\n",
    "\n",
    "## Connecting to GitHub\n",
    "- to clone a repo on a server (using ssh) using GitHub follow the tutorial [here](https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/github-clone-with-ssh-keys)\n",
    "- remember to save and store all public or private keys within your home directory\n",
    "\n",
    "## Transferring files\n",
    "### Using VsCode\n",
    "- drag files into the working tab directory in VsCode\n",
    "- probably the best way to quickly transfer files to the Cloud\n",
    "### Using Github\n",
    "- this should be straight forward as you can just clone your project in your local directory\n",
    "### Using CyberDuck (Mac)\n",
    "- download on the internet, have to pay via app store\n",
    "- connect via FTAP and there's no need to change the port number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up ipynb file for Tuning\n",
    "\n",
    "Once everything is set up on the server or on your own machine, you want to firstly set/assign what GPUs we're going to be using in the cluster. We need to do this first before setting up everything else so it restricts anything from using other GPU's. Check the [usage](https://hex.cs.bath.ac.uk/usage) of the current cluster you're working on and change the GPUs you're using based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important to run this first or else GPU allocation will not work\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding all of the neccessary imports\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, TrainingArguments, pipeline, BitsAndBytesConfig\n",
    "from huggingface_hub import login, snapshot_download, hf_hub_download\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "import torch.nn as nn\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if GPU allocation is successful\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check VRAM\n",
    "# you will need to alter this code if you're not using the Cheery cluster on Hex\n",
    "def check_vram():\n",
    "    # Initialize NVIDIA management library\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "    # Get a handle for each GPU device\n",
    "    handle_list = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(pynvml.nvmlDeviceGetCount())]\n",
    "\n",
    "    info_used = []\n",
    "    points = [0,1,2,3,4,5,6,7]\n",
    "    max_vram = 24\n",
    "\n",
    "    # Iterate over all GPU devices and print VRAM usage\n",
    "    for handle in handle_list:\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        info_used.append( (info.used//1024**2)/1000)\n",
    "\n",
    "    print (info_used)\n",
    "\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.ylim(0,max_vram)\n",
    "    plt.bar(points, info_used)\n",
    "    plt.plot()\n",
    "    pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the model\n",
    "For this example we're going to be fine tuning the meta llama-2-7b model that's being downloaded from huggingface. In order to do this though you'll need to go through some preliminary steps\n",
    "\n",
    "- request access to the llama-2-7b (recommended to get llama-2-7b-hf so you don't have to convert it later) model. This should take about an hour or so depending on when you've sent the request\n",
    "- create an access key for your huggingface account. Follow this tutorial [here](https://huggingface.co/docs/hub/en/security-tokens) for more info. Make sure that you give the key read and write permissions\n",
    "- upload your key into a txt file and store it onto the server. As shown in this test it's in the project directory which is not good practice. Store it in your home directory and replace the path\n",
    "- if you need to convert your model to the huggingface (hf) version, run the `convert_llama_weights_to_hf.py`. This will only work for llama models\n",
    "\n",
    "If all steps are done then the next code block should work smoothly but will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT RUN THIS CODE BLOCK IF 'llama-2-7b-hf' IS IN YOUR DIRECTORY\n",
    "\"\"\"\n",
    "\n",
    "# getting the authorisation from huggingface\n",
    "access_key = open('hf_ak.txt','r').read()\n",
    "login(token = access_key)\n",
    "\n",
    "# use df -H in the terminal to check and see if there's enough space to download the model\n",
    "# will save the model in the directory specified\n",
    "# for future cases it's recommended to download \"meta-llama/Llama-2-7b-hf\" instead of \"meta-llama/Llama-2-7b\" as you will not need to convert it later\n",
    "model_path = snapshot_download(\"meta-llama/Llama-2-7b-hf\", local_dir=\"./llama-2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do not run any of this is if llama-2-7b-hf is filled\n",
    "- You'll need to convert them to the huggingface Transformers format using the conversion script `convert_llama_weights_to_hf.py`. \n",
    "- Obviously, hf stands for huggingface. Maybe with the hf version, the conversion wouldn't be needed\n",
    "- to run this, run the following\n",
    "\n",
    "```\n",
    "python convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /llama-2-7b --model_size 7B --output_dir /llama-2-7b-hf\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each GPU util\n",
    "!nvidia-smi --query-gpu=utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"llama-2-7b-hf\"\n",
    "new_model_name = \"llama-2-7b-enhanced\" #You can give your own name for fine tuned model\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use the following code if you want to unload the VRAM. You may have to run this more than once to unload\n",
    "Remove model from GPU. Add any more variables that can get loaded on\n",
    "\"\"\"\n",
    "\n",
    "# uncomment this if not deleted\n",
    "# del base_model\n",
    "# del llama_tokenizer\n",
    "\n",
    "# Clear any remaining CUDA memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if your project is still loaded onto the VRAM\n",
    "print(torch.cuda.memory_allocated())\n",
    "# this should return 0 if everything is unloaded\n",
    "print(torch.cuda.memory_reserved())\n",
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "As we're already connected to our huggingface account, you can also download datasets from there too which is what we're doing. This dataset specifically is made for the llama 2 models. \n",
    "\n",
    "You need to make sure the data is configured to the data you want. If you want to use data that has no configuration for your chosen model, then reconfigure the data. I will not be doing that in this file but I'll leave a link to example code if I get round to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set\n",
    "# only 1K datapoints but each points has a lot of data\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "# check the data\n",
    "print(training_data.shape)\n",
    "# #11 is a QA sample in English\n",
    "print(training_data[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Once you have your model and the data you want to fine tune the model with, we'll start training. In this test, we are going to be using LoRa to make training a language model possible in the first place.\n",
    "\n",
    "## How LoRa Works:\n",
    "Full paper [here](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "LoRa works by reducing the amount of weight we're fine tuning so that large models can be able to fit onto smaller machines. It does this by using 2 matricies of weights with varying size (rank or r) to represent a hidden layer\n",
    "\n",
    "For example, if I want to train a single hidden layer with input size 400 and output size 600, then I can reduce that using 2 matrices: a with size `[input, rank]` and b with size `[rank, output]`. Therefore, if I get the dot product of ab, then the size would be `[input, ouput]`. Depending on what you set the rank to be (including other variables). This can significantly reduce the amount of weights that you're fine tuning. Obviously the less weights you're fine tuning, the less effect fine tuning has on the model. This can be good so you don't dramatically alter the pretraining but bad if it's not altering enough.\n",
    "\n",
    "Once the fine tuning is complete, then you add the fine tuned weights to the weights in the model which will give you your fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "# LoRA Config\n",
    "# reduce rank r if you're running out of vram\n",
    "peft_parameters = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with LoRA configuration\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the model\n",
    "\n",
    "Since the model that we saved is only the fine tuned weights without the pretrained ones, we need to merge them both together and save it. If llama-2-7b-merged is already in your directory, you will not need to do this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "# make sure that both models are available before running or else inference will not work\n",
    "base_model_name = \"llama-2-7b-hf\"\n",
    "new_model_name = \"llama-2-7b-enhanced\"\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"llama-2-7b-merged\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference / Testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(\n",
    "    model_path: str,\n",
    "    texts: List[str],\n",
    "    max_tokens: Optional[int] = None,\n",
    "    sliding_window: Optional[int] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity using VLLM for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the merged model\n",
    "        texts: List of texts to evaluate\n",
    "        max_tokens: Maximum sequence length (optional)\n",
    "        sliding_window: Size of sliding window for attention (optional)\n",
    "    \n",
    "    Returns:\n",
    "        float: Average perplexity across all texts\n",
    "    \"\"\"\n",
    "    # Initialize VLLM with your model\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=1,  # Adjust based on your GPU setup\n",
    "        max_num_seqs=1,\n",
    "        max_num_batched_tokens=4096,\n",
    "        sliding_window=sliding_window\n",
    "    )\n",
    "    \n",
    "    # Set sampling parameters for getting logprobs\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=1,  # We only need one token for next-token prediction\n",
    "        logprobs=True\n",
    "    )\n",
    "    \n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        # Get logprobs for each position in the sequence\n",
    "        outputs = llm.generate([text], sampling_params)\n",
    "        \n",
    "        for output in outputs:\n",
    "            # Get the logprobs for each token\n",
    "            logprobs = output.outputs[0].logprobs[0]\n",
    "            \n",
    "            # Sum negative log likelihood\n",
    "            total_nll += -sum(logprobs)\n",
    "            total_tokens += len(logprobs)\n",
    "    \n",
    "    # Calculate average negative log likelihood\n",
    "    avg_nll = total_nll / total_tokens\n",
    "    \n",
    "    # Convert to perplexity\n",
    "    perplexity = np.exp(avg_nll)\n",
    "    \n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the perplexity measure\n",
    "\n",
    "\n",
    "# can replace this with test data\n",
    "test_texts = [\n",
    "        \"This is a sample text to evaluate.\",\n",
    "        \"Another example for testing perplexity.\"\n",
    "    ]\n",
    "\n",
    "ppl = calculate_perplexity(\n",
    "        model_path=model_path,\n",
    "        texts=test_texts,\n",
    "        max_tokens=2048,  # Adjust based on your model's context window\n",
    "        sliding_window=512  # Optional: use if your model supports attention sliding window\n",
    "    )\n",
    "    \n",
    "print(f\"Average perplexity: {ppl:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
