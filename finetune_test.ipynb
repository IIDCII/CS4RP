{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Test \n",
    "Making a test for fine tuning the models as this is my first time doing so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\n",
    "\n",
    "# peft_utils has been depricated, make sure that new import is correct\n",
    "# from torchtune.modules.peft.peft_utils import get_adapter_params, set_trainable_params\n",
    "from torchtune.modules.peft import get_adapter_params, set_trainable_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- default settings for both of the models will be the same\n",
    "- have to define which layers lora will be applied to\n",
    "- therefore in the example below, we're applying lora to just the query and key in every attention module\n",
    "- can also apply it to other linear layers within the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = llama2_7b()\n",
    "\n",
    "# appplying lora to the query and the value projection\n",
    "lora_llama2_7b = lora_llama2_7b(lora_attn_modules=[\"q_proj\",\"v_proj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can see in the following print statement that the input and output features are a dot product of [original_in,lora_rank] and [lora_rank,original_out]. Getting the dot product of it would get you the same matrix shape with using a lot less elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n",
      "MultiHeadAttention(\n",
      "  (q_proj): LoRALinear(\n",
      "    (dropout): Identity()\n",
      "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): LoRALinear(\n",
      "    (dropout): Identity()\n",
      "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model.layers[0].attn)\n",
    "print(lora_llama2_7b.layers[0].attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.attn.q_proj.lora_a.weight', 'layers.0.attn.q_proj.lora_b.weight', 'layers.0.attn.v_proj.lora_a.weight', 'layers.0.attn.v_proj.lora_b.weight', 'layers.1.attn.q_proj.lora_a.weight', 'layers.1.attn.q_proj.lora_b.weight', 'layers.1.attn.v_proj.lora_a.weight', 'layers.1.attn.v_proj.lora_b.weight', 'layers.2.attn.q_proj.lora_a.weight', 'layers.2.attn.q_proj.lora_b.weight', 'layers.2.attn.v_proj.lora_a.weight', 'layers.2.attn.v_proj.lora_b.weight', 'layers.3.attn.q_proj.lora_a.weight', 'layers.3.attn.q_proj.lora_b.weight', 'layers.3.attn.v_proj.lora_a.weight', 'layers.3.attn.v_proj.lora_b.weight', 'layers.4.attn.q_proj.lora_a.weight', 'layers.4.attn.q_proj.lora_b.weight', 'layers.4.attn.v_proj.lora_a.weight', 'layers.4.attn.v_proj.lora_b.weight', 'layers.5.attn.q_proj.lora_a.weight', 'layers.5.attn.q_proj.lora_b.weight', 'layers.5.attn.v_proj.lora_a.weight', 'layers.5.attn.v_proj.lora_b.weight', 'layers.6.attn.q_proj.lora_a.weight', 'layers.6.attn.q_proj.lora_b.weight', 'layers.6.attn.v_proj.lora_a.weight', 'layers.6.attn.v_proj.lora_b.weight', 'layers.7.attn.q_proj.lora_a.weight', 'layers.7.attn.q_proj.lora_b.weight', 'layers.7.attn.v_proj.lora_a.weight', 'layers.7.attn.v_proj.lora_b.weight', 'layers.8.attn.q_proj.lora_a.weight', 'layers.8.attn.q_proj.lora_b.weight', 'layers.8.attn.v_proj.lora_a.weight', 'layers.8.attn.v_proj.lora_b.weight', 'layers.9.attn.q_proj.lora_a.weight', 'layers.9.attn.q_proj.lora_b.weight', 'layers.9.attn.v_proj.lora_a.weight', 'layers.9.attn.v_proj.lora_b.weight', 'layers.10.attn.q_proj.lora_a.weight', 'layers.10.attn.q_proj.lora_b.weight', 'layers.10.attn.v_proj.lora_a.weight', 'layers.10.attn.v_proj.lora_b.weight', 'layers.11.attn.q_proj.lora_a.weight', 'layers.11.attn.q_proj.lora_b.weight', 'layers.11.attn.v_proj.lora_a.weight', 'layers.11.attn.v_proj.lora_b.weight', 'layers.12.attn.q_proj.lora_a.weight', 'layers.12.attn.q_proj.lora_b.weight', 'layers.12.attn.v_proj.lora_a.weight', 'layers.12.attn.v_proj.lora_b.weight', 'layers.13.attn.q_proj.lora_a.weight', 'layers.13.attn.q_proj.lora_b.weight', 'layers.13.attn.v_proj.lora_a.weight', 'layers.13.attn.v_proj.lora_b.weight', 'layers.14.attn.q_proj.lora_a.weight', 'layers.14.attn.q_proj.lora_b.weight', 'layers.14.attn.v_proj.lora_a.weight', 'layers.14.attn.v_proj.lora_b.weight', 'layers.15.attn.q_proj.lora_a.weight', 'layers.15.attn.q_proj.lora_b.weight', 'layers.15.attn.v_proj.lora_a.weight', 'layers.15.attn.v_proj.lora_b.weight', 'layers.16.attn.q_proj.lora_a.weight', 'layers.16.attn.q_proj.lora_b.weight', 'layers.16.attn.v_proj.lora_a.weight', 'layers.16.attn.v_proj.lora_b.weight', 'layers.17.attn.q_proj.lora_a.weight', 'layers.17.attn.q_proj.lora_b.weight', 'layers.17.attn.v_proj.lora_a.weight', 'layers.17.attn.v_proj.lora_b.weight', 'layers.18.attn.q_proj.lora_a.weight', 'layers.18.attn.q_proj.lora_b.weight', 'layers.18.attn.v_proj.lora_a.weight', 'layers.18.attn.v_proj.lora_b.weight', 'layers.19.attn.q_proj.lora_a.weight', 'layers.19.attn.q_proj.lora_b.weight', 'layers.19.attn.v_proj.lora_a.weight', 'layers.19.attn.v_proj.lora_b.weight', 'layers.20.attn.q_proj.lora_a.weight', 'layers.20.attn.q_proj.lora_b.weight', 'layers.20.attn.v_proj.lora_a.weight', 'layers.20.attn.v_proj.lora_b.weight', 'layers.21.attn.q_proj.lora_a.weight', 'layers.21.attn.q_proj.lora_b.weight', 'layers.21.attn.v_proj.lora_a.weight', 'layers.21.attn.v_proj.lora_b.weight', 'layers.22.attn.q_proj.lora_a.weight', 'layers.22.attn.q_proj.lora_b.weight', 'layers.22.attn.v_proj.lora_a.weight', 'layers.22.attn.v_proj.lora_b.weight', 'layers.23.attn.q_proj.lora_a.weight', 'layers.23.attn.q_proj.lora_b.weight', 'layers.23.attn.v_proj.lora_a.weight', 'layers.23.attn.v_proj.lora_b.weight', 'layers.24.attn.q_proj.lora_a.weight', 'layers.24.attn.q_proj.lora_b.weight', 'layers.24.attn.v_proj.lora_a.weight', 'layers.24.attn.v_proj.lora_b.weight', 'layers.25.attn.q_proj.lora_a.weight', 'layers.25.attn.q_proj.lora_b.weight', 'layers.25.attn.v_proj.lora_a.weight', 'layers.25.attn.v_proj.lora_b.weight', 'layers.26.attn.q_proj.lora_a.weight', 'layers.26.attn.q_proj.lora_b.weight', 'layers.26.attn.v_proj.lora_a.weight', 'layers.26.attn.v_proj.lora_b.weight', 'layers.27.attn.q_proj.lora_a.weight', 'layers.27.attn.q_proj.lora_b.weight', 'layers.27.attn.v_proj.lora_a.weight', 'layers.27.attn.v_proj.lora_b.weight', 'layers.28.attn.q_proj.lora_a.weight', 'layers.28.attn.q_proj.lora_b.weight', 'layers.28.attn.v_proj.lora_a.weight', 'layers.28.attn.v_proj.lora_b.weight', 'layers.29.attn.q_proj.lora_a.weight', 'layers.29.attn.q_proj.lora_b.weight', 'layers.29.attn.v_proj.lora_a.weight', 'layers.29.attn.v_proj.lora_b.weight', 'layers.30.attn.q_proj.lora_a.weight', 'layers.30.attn.q_proj.lora_b.weight', 'layers.30.attn.v_proj.lora_a.weight', 'layers.30.attn.v_proj.lora_b.weight', 'layers.31.attn.q_proj.lora_a.weight', 'layers.31.attn.q_proj.lora_b.weight', 'layers.31.attn.v_proj.lora_a.weight', 'layers.31.attn.v_proj.lora_b.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading base model weights onto the lora model\n",
    "# should not be a problem at all to do in the first place\n",
    "lora_llama2_7b.load_state_dict(base_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  6742609920 total params,\n",
      "  4194304\" trainable params,\n",
      "  0.06% of all params are trainable.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# setting lora params to be trainable\n",
    "lora_params = get_adapter_params(lora_llama2_7b)\n",
    "\n",
    "set_trainable_params(lora_llama2_7b, lora_params)\n",
    "\n",
    "# numel gets the total number of random elements\n",
    "total_params  = sum([p.numel() for p in lora_llama2_7b.parameters()])\n",
    "trainable_params = sum([p.numel() for p in lora_llama2_7b.parameters() if p.requires_grad])\n",
    "\n",
    "print(\n",
    "  f\"\"\"\n",
    "  {total_params} total params,\n",
    "  {trainable_params}\" trainable params,\n",
    "  {(100.0 * trainable_params / total_params):.2f}% of all params are trainable.\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4rp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
