{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Test \n",
    "Test for fine tuning language models using LoRa on Bath's Hex Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up on Hex Cloud\n",
    "## Getting Access\n",
    "- if you don't have permission to a cluster/machine, email Tom Haines\n",
    "- open a terminal and sign in via ssh and input your password\n",
    "    ```\n",
    "    ssh uniusername@clustername.cs.bath.ac.uk\n",
    "    ```\n",
    "    If the following doesn't work, make sure that you're in Bath and if not, connect to the university's VPN or any other VPN\n",
    "\n",
    "## Access project folders\n",
    "- once you're in you want to navigate to the the fast/er folders. You want to do all of your projects there because they're the least laggy and you have up to 3.8T of storage for your project (varies)\n",
    "\n",
    "- <img src=\"assets/s1.png\" width = \"300\">\n",
    "\n",
    "- the following is an example of how you would access the fast/faster project files\n",
    "- check which of the files has the most space and create a folder with your username\n",
    "- do not put any confidential information or API keys within this folder. If you want to use that, put that in your home folder and call them from your project folder\n",
    "\n",
    "## Getting Access via VsCode\n",
    "- open VsCode and in the bottom left corner, you'll get the option to open a remote connection\n",
    "- then select `Connect Current Window to Host...`\n",
    "- if the configured ssh you want to use is not there, then select `Add New SSH Host`\n",
    "    - if you've selected add new host, input `uniusername@clustername.cs.bath.ac.uk`\n",
    "    - else, select the ssh connection you want\n",
    "- if you're experiencing issues make sure that you're either in uni or you have a VPN turned on\n",
    "\n",
    "## Connecting to GitHub\n",
    "- to clone a repo on a server (using ssh) using GitHub follow the tutorial [here](https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/github-clone-with-ssh-keys)\n",
    "- remember to save and store all public or private keys within your home directory\n",
    "\n",
    "## Transferring files\n",
    "### Using VsCode\n",
    "- drag files into the working tab directory in VsCode\n",
    "- probably the best way to quickly transfer files to the Cloud\n",
    "### Using Github\n",
    "- this should be straight forward as you can just clone your project in your local directory\n",
    "### Using CyberDuck (Mac)\n",
    "- download on the internet, have to pay via app store\n",
    "- connect via FTAP and there's no need to change the port number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up ipynb file for Tuning\n",
    "\n",
    "Once everything is set up on the server or on your own machine, you want to firstly set/assign what GPUs we're going to be using in the cluster. We need to do this first before setting up everything else so it restricts anything from using other GPU's. Check the [usage](https://hex.cs.bath.ac.uk/usage) of the current cluster you're working on and change the GPUs you're using based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important to run this first or else GPU allocation will not work\n",
    "import os\n",
    "# set the GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "# setting for vllm inference so that it can run in parallel\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding all of the neccessary imports\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, TrainingArguments, pipeline, BitsAndBytesConfig\n",
    "from huggingface_hub import login, snapshot_download, hf_hub_download\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "import torch.nn as nn\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Optional\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "# checking if GPU allocation is successful\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.079, 5.791, 0.415, 4.677, 0.418, 0.418, 0.415, 0.415]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check VRAM\n",
    "# you will need to alter this code if you're not using the Cheery cluster on Hex\n",
    "def check_vram():\n",
    "    # Initialize NVIDIA management library\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "    # Get a handle for each GPU device\n",
    "    handle_list = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(pynvml.nvmlDeviceGetCount())]\n",
    "\n",
    "    info_used = []\n",
    "    points = [0,1,2,3,4,5,6,7]\n",
    "    max_vram = 24\n",
    "\n",
    "    # Iterate over all GPU devices and print VRAM usage\n",
    "    for handle in handle_list:\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        info_used.append( (info.used//1024**2)/1000)\n",
    "\n",
    "    print (info_used)\n",
    "\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.ylim(0,max_vram)\n",
    "    plt.bar(points, info_used)\n",
    "    plt.plot()\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the model\n",
    "For this example we're going to be fine tuning the meta llama-2-7b model that's being downloaded from huggingface. In order to do this though you'll need to go through some preliminary steps\n",
    "\n",
    "- request access to the llama-2-7b (recommended to get llama-2-7b-hf so you don't have to convert it later) model. This should take about an hour or so depending on when you've sent the request\n",
    "- create an access key for your huggingface account. Follow this tutorial [here](https://huggingface.co/docs/hub/en/security-tokens) for more info. Make sure that you give the key read and write permissions\n",
    "- upload your key into a txt file and store it onto the server. As shown in this test it's in the project directory which is not good practice. Store it in your home directory and replace the path\n",
    "- if you need to convert your model to the huggingface (hf) version, run the `convert_llama_weights_to_hf.py`. This will only work for llama models\n",
    "\n",
    "If all steps are done then the next code block should work smoothly but will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT RUN THIS CODE BLOCK IF 'llama-2-7b-hf' IS IN YOUR DIRECTORY\n",
    "\"\"\"\n",
    "\n",
    "# getting the authorisation from huggingface\n",
    "access_key = open('hf_ak.txt','r').read()\n",
    "login(token = access_key)\n",
    "\n",
    "# use df -H in the terminal to check and see if there's enough space to download the model\n",
    "# will save the model in the directory specified\n",
    "# for future cases it's recommended to download \"meta-llama/Llama-2-7b-hf\" instead of \"meta-llama/Llama-2-7b\" as you will not need to convert it later\n",
    "model_path = snapshot_download(\"meta-llama/Llama-2-7b-hf\", local_dir=\"./llama-2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- do not run any of this is if llama-2-7b-hf is filled\n",
    "- You'll need to convert them to the huggingface Transformers format using the conversion script `convert_llama_weights_to_hf.py`. \n",
    "- Obviously, hf stands for huggingface. Maybe with the hf version, the conversion wouldn't be needed\n",
    "- to run this, run the following\n",
    "\n",
    "```\n",
    "python convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /llama-2-7b --model_size 7B --output_dir /llama-2-7b-hf\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each GPU util\n",
    "!nvidia-smi --query-gpu=utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"llama-2-7b-hf\"\n",
    "new_model_name = \"llama-2-7b-enhanced-test\" #You can give your own name for fine tuned model\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use the following code if you want to unload the VRAM. You may have to run this more than once to unload\n",
    "Remove model from GPU. Add any more variables that can get loaded on\n",
    "\"\"\"\n",
    "\n",
    "# uncomment this if not deleted\n",
    "# del base_model\n",
    "# del llama_tokenizer\n",
    "\n",
    "# Clear any remaining CUDA memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if your project is still loaded onto the VRAM\n",
    "print(torch.cuda.memory_allocated())\n",
    "# this should return 0 if everything is unloaded\n",
    "print(torch.cuda.memory_reserved())\n",
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "As we're already connected to our huggingface account, you can also download datasets from there too which is what we're doing. This dataset specifically is made for the llama 2 models. \n",
    "\n",
    "You need to make sure the data is configured to the data you want. If you want to use data that has no configuration for your chosen model, then reconfigure the data. I will not be doing that in this file but I'll leave a link to example code if I get round to it\n",
    "\n",
    "For the llama 2 data formatting, the data is classed under `text` and starts on the `<s>` token and ends on the `</s>` token. `[INST]` implies the query and `[/INST]` implies the response. As long as you have data following this format, fine tuning should be able to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set\n",
    "# only 1K datapoints but each points has a lot of data\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# note this dataset only has a test split and no test one\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "# check the data\n",
    "print(training_data.shape)\n",
    "print(type(training_data))\n",
    "# #11 is a QA sample in English\n",
    "print(training_data[11])\n",
    "print(training_data[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your tokenizer and dataset loaded\n",
    "total_tokens = 0\n",
    "for item in training_data:\n",
    "    tokens = llama_tokenizer(item['text'], return_tensors='pt')\n",
    "    total_tokens += len(tokens['input_ids'][0])\n",
    "\n",
    "print(f\"Total tokens in dataset: {total_tokens}\")\n",
    "print(f\"Average tokens per example: {total_tokens / len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Once you have your model and the data you want to fine tune the model with, we'll start training. In this test, we are going to be using LoRa to make training a language model possible in the first place.\n",
    "\n",
    "## How LoRa Works:\n",
    "Full paper [here](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "LoRa works by reducing the amount of weight we're fine tuning so that large models can be able to fit onto smaller machines. It does this by using 2 matricies of weights with varying size (rank or r) to represent a hidden layer\n",
    "\n",
    "For example, if I want to train a single hidden layer with input size 400 and output size 600, then I can reduce that using 2 matrices: a with size `[input, rank]` and b with size `[rank, output]`. Therefore, if I get the dot product of ab, then the size would be `[input, ouput]`. Depending on what you set the rank to be (including other variables). This can significantly reduce the amount of weights that you're fine tuning. Obviously the less weights you're fine tuning, the less effect fine tuning has on the model. This can be good so you don't dramatically alter the pretraining but bad if it's not altering enough.\n",
    "\n",
    "Once the fine tuning is complete, then you add the fine tuned weights to the weights in the model which will give you your fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "# LoRA Config\n",
    "# reduce rank r if you're running out of vram\n",
    "peft_parameters = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with LoRA configuration\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the model\n",
    "\n",
    "Since the model that we saved is only the fine tuned weights without the pretrained ones, we need to merge them both together and save it. If llama-2-7b-merged is already in your directory, you will not need to do this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "# make sure that both models are available before running or else inference will not work\n",
    "base_model_name = \"llama-2-7b-hf\"\n",
    "new_model_name = \"llama-2-7b-enhanced\"\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"llama-2-7b-merged\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the perplexity without vllm\n",
    "# seems right now that sampling params from vllm doesn't support logprobs for the prompt given\n",
    "\n",
    "# getting pp\n",
    "def calculate_perplexity(model, tokenizer, text, max_length=300):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a text using a language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        text: Input text to evaluate\n",
    "        max_length: Maximum sequence length to process\n",
    "        \n",
    "    Returns:\n",
    "        float: The perplexity score\n",
    "    \"\"\"\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    \n",
    "    # Get input IDs and create target labels (shifted by 1)\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "    \n",
    "    # Calculate loss with no gradient tracking\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs.loss\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "    loss = neg_log_likelihood\n",
    "    return ppl.item(), loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(model, tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Calculate average perplexity across multiple texts.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        texts: List of texts to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        float: Average perplexity across all texts\n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    total_loss = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            ppl_and_loss = calculate_perplexity(model, tokenizer, text)\n",
    "            ppl = ppl_and_loss[0]\n",
    "            loss = ppl_and_loss[1]\n",
    "\n",
    "            perplexities.append(ppl)\n",
    "            total_loss.append(loss)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return perplexities, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548fb5e943e445dca36d9f09ba7e5645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity:  102.98766326904297\n",
      "[45.90740966796875, 160.0679168701172]\n",
      "Average loss:  4.4511123\n",
      "[tensor(3.8266), tensor(5.0756)]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"llama-2-7b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    # need to \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "test_set = [\n",
    "    \"<s>[INST] Hello, how are you? [/INST] I'm doing fine thank you, I hope you're doing well too!</s>:\",\n",
    "    \"<s>[INST] Are you real?[/INST] I'm a model made out of billions of parameters so that I can form sentences like this.</s>:\",\n",
    "]\n",
    "\n",
    "ppl_and_loss = evaluate_dataset(model, tokenizer, test_set)\n",
    "\n",
    "ppl = ppl_and_loss[0]\n",
    "loss = ppl_and_loss[1]\n",
    "\n",
    "print (\"Average perplexity: \", np.mean(ppl))\n",
    "print (ppl)\n",
    "\n",
    "print (\"Average loss: \", np.mean(loss))\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "if the models that you want to test are already saved, then just run the os environment setup,import and nvm check blocks and you can head straight down here\n",
    "\n",
    "For the test we're going to be gathering the perplexity score of the model based on the test data and the responses it's made to the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atm this just gets the confidence and the response\n",
    "# change all of the pp notation to confidence\n",
    "def get_response(\n",
    "    model_path: str,\n",
    "    tokenizer_path: str,\n",
    "    texts: List[str],\n",
    "    max_tokens: Optional[int] = None,\n",
    "    sliding_window: Optional[int] = None,\n",
    "    max_new_tokens: int = 100,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity using VLLM for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the merged model\n",
    "        texts: List of texts to evaluate\n",
    "        max_tokens: Maximum sequence length (optional)\n",
    "        sliding_window: Size of sliding window for attention (optional)\n",
    "    \n",
    "    Returns:\n",
    "        float: Average perplexity across all texts\n",
    "    \"\"\"\n",
    "    # Initialize VLLM with your model\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=2,  # Adjust based on your GPU setup\n",
    "        max_num_seqs=1,\n",
    "        tokenizer = tokenizer_path,\n",
    "        trust_remote_code= True,\n",
    "    )\n",
    "    \n",
    "    # Set sampling parameters for generating responses\n",
    "    gen_sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_tokens=max_new_tokens,\n",
    "        # change this for the llama tokens\n",
    "        stop=[\"</s>\"],\n",
    "        logprobs=True,\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    con_sum = 0\n",
    "    con_total = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        # Generate response\n",
    "        gen_outputs = llm.generate([text], gen_sampling_params)\n",
    "        response = gen_outputs[0].outputs[0].text\n",
    "\n",
    "        # get pp\n",
    "        cum_logprob = gen_outputs[0].outputs[0].cumulative_logprob\n",
    "        tokens_length = len(gen_outputs[0].outputs[0].token_ids)\n",
    "        confidence = np.exp(- cum_logprob/tokens_length)\n",
    "        \n",
    "        # Store results for this text\n",
    "        result = {\n",
    "            \"input\": text,\n",
    "            \"response\": response,\n",
    "            \"perplexity\": confidence,\n",
    "            \"output\": gen_outputs,\n",
    "        }\n",
    "        results.append(result)\n",
    "        con_sum += confidence \n",
    "    \n",
    "    con_total = con_sum / len(texts)\n",
    "    \n",
    "    # Calculate average perplexity across all texts\n",
    "    return con_total , results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this code for vllm inference\n",
    "\n",
    "# run the perplexity measure\n",
    "model_path = \"llama-2-7b-merged\"\n",
    "tokenizer_path = \"llama-2-7b-merged\"\n",
    "\n",
    "# can replace this with test data\n",
    "# in the training data but just using this example for testing purposes\n",
    "prompts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Are you real?\",\n",
    "]\n",
    "\n",
    "con_and_response = get_response(model_path, tokenizer_path, prompts)\n",
    "confidence = con_and_response[0]\n",
    "response = con_and_response[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---\"*10)\n",
    "for i in response:    \n",
    "    print(\"prompt: \", i[\"input\"])\n",
    "    print(\"---\"*5)\n",
    "    print(\"answer: \", i[\"response\"])\n",
    "    print(\"---\"*5)\n",
    "print(\"overall response confidence: \", confidence)\n",
    "print(\"---\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
